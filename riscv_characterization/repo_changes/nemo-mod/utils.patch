--- nemo/utils.py	2023-10-03 20:26:14.199885393 +0200
+++ nemo-mod/utils.py	2023-10-03 20:19:22.535289315 +0200
@@ -73,17 +73,21 @@
         os.mkdir('checkpoint')
     torch.save(state, './checkpoint/%s.pth' % (checkpoint_name))
 
-def export_onnx(file_name, net, net_inner, input_shape, round_params=True, perm=None, redefine_names=False, batch_size=1, verbose=False):
+def export_onnx(file_name, net, net_inner, input_shape, device, round_params=True, perm=None, redefine_names=False, batch_size=1, verbose=False):
     if perm is None:
         perm = lambda x : x
     pattern = re.compile('[\W_]+')
-    dummy_input = perm(torch.randn(batch_size, *input_shape, device='cuda' if torch.cuda.is_available() else 'cpu'))
+    if (device is not None):
+        dummy_input = perm(torch.randn(batch_size, *input_shape, device=device))
+    else:
+        dummy_input = perm(torch.randn(batch_size, *input_shape, device='cuda' if torch.cuda.is_available() else 'cpu'))
     net.eval()
     # rounding of parameters to avoid strange numerical errors on writeout
     if round_params:
-        for param in net_inner.parameters(recurse=True):
-            if param.dtype is torch.float32:
-                param[:] = torch.round(param)
+        with torch.no_grad():
+            for param in net_inner.parameters(recurse=True):
+                if param.dtype is torch.float32:
+                    param[:] = torch.round(param)
     if redefine_names:
         input_names  = [ 'input' ] + [ pattern.sub('_', n) for n,_ in net_inner.named_parameters() ]
         output_names = [ 'output' ]
